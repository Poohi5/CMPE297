{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MetaTaskLearning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOvJFqC+qORHOZWXZ8li3B+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poohi5/CMPE297/blob/master/Assignment_4/MetaTaskLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOE9hnOUmTiW"
      },
      "source": [
        "#### Mount drive for getting omniglot data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJRbOc8TGRv",
        "outputId": "bfa11d78-ba3c-431a-d444-c42138d47c0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/DLAssignment/data (1)\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-r0oVGpfWbx",
        "outputId": "46e9689e-836c-4da5-d893-5bc274e98f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.33.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ4y-CDZYXQv",
        "outputId": "2d8a2fa6-3126-4346-bf26-b2e1dabcfb93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj6_beQnmYmH"
      },
      "source": [
        "#### Plotting one character from Japanese alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCLSmTeDYxas",
        "outputId": "7aa3ea8a-a991-456c-e7a2-37ca1ce0e162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "Image.open('/content/gdrive/My Drive/DLAssignment/data (1)/images/Japanese_(katakana)/character13/0608_01.png')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAqUlEQVR4nO3OMQ6CQBCF4X9wEyi3tOQYlhQeywKiByOehNJSuzUhjIXE8DDGimjBVPvlzeyMOZNqMqRWvvP0+zM8wqBpdTOz4kWPQA8EuJt8lb4uyt19wgQegIDhnCfNx+drGPcexiTKz70uSqXwqmlXCdta2GhzJvQgHAphj5AovJTCrhK2s9lauNF0r9zNZv+ebmYWNU2fmkvlFqAAAgC5S7rczSuX5gMYvSMsiQG6eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F745E60AB38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnotlxoIbTjH"
      },
      "source": [
        "image_name = '/content/gdrive/My Drive/DLAssignment/data (1)/images/Sanskrit/character13/0863_13.png'\n",
        "alphabet, character, rotation = 'Sanskrit/character13/rot000'.split('/')\n",
        "rotation = float(rotation[3:])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD6rtrdVmefG"
      },
      "source": [
        "#### Converting Image into an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1terQjD6mkm",
        "outputId": "be3cafd4-73b5-42d5-df8d-882966f069f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(Image.open(image_name).rotate(rotation).resize((28, 28)), np.float32,copy=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMrMiGlbmlUU"
      },
      "source": [
        "#### Splitting details in the /data/omniglot/splits/train.txt file which has the language name, character number, rotation information and images in /data/omniglot/data/ directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYatqgZG6pWl"
      },
      "source": [
        "train_split_path = os.path.join(root_dir, 'splits', 'train.txt')\n",
        "\n",
        "with open(train_split_path, 'r') as train_split:\n",
        "    train_classes = [line.rstrip() for line in train_split.readlines()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OXVibg061zv"
      },
      "source": [
        "#number of classes\n",
        "no_of_classes = len(train_classes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku5uAIXP654q"
      },
      "source": [
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "#image width\n",
        "img_width = 28\n",
        "\n",
        "#image height\n",
        "img_height = 28\n",
        "channels = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3URz5UqcmrjC"
      },
      "source": [
        "####  Initialize the training dataset with a shape as a number of classes, number of examples, image height and image width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJZtfVXs68GQ"
      },
      "source": [
        "train_dataset = np.zeros([no_of_classes, num_examples, img_height, img_width], dtype=np.float32)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF1AEZBAmxjo"
      },
      "source": [
        "#### Read all the images, convert it to numpy array and store it our train_dataset array with their label and values, that is, train_dataset = [label, values]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF1DDW-C6-0Q"
      },
      "source": [
        "for label, name in enumerate(train_classes):\n",
        "    alphabet, character, rotation = name.split('/')\n",
        "    rotation = float(rotation[3:])\n",
        "    img_dir = os.path.join(root_dir, 'data', alphabet, character)\n",
        "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "  \n",
        "    \n",
        "    for index, img_file in enumerate(img_files):\n",
        "        values = 1. - np.array(Image.open(img_file).rotate(rotation).resize((img_width, img_height)), np.float32, copy=False)\n",
        "        train_dataset[label, index] = values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfbCuQhv7BaY",
        "outputId": "8c3a9901-97b6-46d5-a90d-4c06d1ef616a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4112, 20, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_glh_S16uu"
      },
      "source": [
        "#### Generate the embeddings using a convolution operation as our inputs are images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWxnTYex7Dkw"
      },
      "source": [
        "def convolution_block(inputs, out_channels, name='conv'):\n",
        "\n",
        "    conv = tf.layers.conv2d(inputs, out_channels, kernel_size=3, padding='SAME')\n",
        "    conv = tf.contrib.layers.batch_norm(conv, updates_collections=None, decay=0.99, scale=True, center=True)\n",
        "    conv = tf.nn.relu(conv)\n",
        "    conv = tf.contrib.layers.max_pool2d(conv, 2)\n",
        "    \n",
        "    return conv"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt3IpqMR2Aa2"
      },
      "source": [
        "#### Embedding comprising of four convolutional blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adTAVOD7FnB"
      },
      "source": [
        "def get_embeddings(support_set, h_dim, z_dim, reuse=False):\n",
        "\n",
        "        net = convolution_block(support_set, h_dim)\n",
        "        net = convolution_block(net, h_dim)\n",
        "        net = convolution_block(net, h_dim) \n",
        "        net = convolution_block(net, z_dim) \n",
        "        net = tf.contrib.layers.flatten(net)\n",
        "        \n",
        "        return net"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ClOm7s-2HX_"
      },
      "source": [
        "### Sample some data points from each class as a support set and train the network using the support set in an episodic fashion, since we are one shot learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwvScMHj7HzZ"
      },
      "source": [
        "#number of classes\n",
        "num_way = 60  \n",
        "\n",
        "#number of examples per class for support set\n",
        "num_shot = 5  \n",
        "\n",
        "#number of query points\n",
        "num_query = 5 \n",
        "\n",
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "h_dim = 64\n",
        "\n",
        "z_dim = 64"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmPgEBaXe6QE"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeiJ3lka2L1w"
      },
      "source": [
        "#### Initialize placeholders for our support set and query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-06OaxN7KJB"
      },
      "source": [
        "support_set = tf.compat.v1.placeholder(tf.float32, [None, None, img_height, img_width, channels])\n",
        "query_set = tf.compat.v1.placeholder(tf.float32, [None, None, img_height, img_width, channels])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BHGYoTU2QAA"
      },
      "source": [
        "#### Store the shape of our support set and query set in support_set_shape and query_set_shape respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWmkW7EQ7NQC"
      },
      "source": [
        "support_set_shape = tf.shape(support_set)\n",
        "query_set_shape = tf.shape(query_set)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN-hAjbIe_-6"
      },
      "source": [
        "num_classes, num_support_points = support_set_shape[0], support_set_shape[1]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKA5PUfTfCdS"
      },
      "source": [
        "num_query_points = query_set_shape[1]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_7kJObK2dTh"
      },
      "source": [
        "#### Define the placeholder for the label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp6gHisCfFGi"
      },
      "source": [
        "y = tf.compat.v1.placeholder(tf.int64, [None, None])\n",
        "\n",
        "#convert the label to one hot\n",
        "y_one_hot = tf.one_hot(y, depth=num_classes)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPpj6cCq2hQ1"
      },
      "source": [
        "#### Generate the embeddings for the support set using the embedding function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwIhTXukfH4i",
        "outputId": "b6b7752c-9746-4569-efce-9af2534d519a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "support_set_embeddings = get_embeddings(tf.reshape(support_set, [num_classes * num_support_points, img_height, img_width, channels]), h_dim, z_dim)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-4ba6064d20c1>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px20piQl2lYQ"
      },
      "source": [
        "#### Compute the prototype of each class which is the mean vector of the support set embeddings of the class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3r42brOfOwT"
      },
      "source": [
        "\n",
        "embedding_dimension = tf.shape(support_set_embeddings)[-1]\n",
        "\n",
        "class_prototype = tf.reduce_mean(tf.reshape(support_set_embeddings, [num_classes, num_support_points, embedding_dimension]), axis=1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00n5nn4t2pts"
      },
      "source": [
        "#### Use the same embedding function for getting embeddings of the query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6dFNu5tf5fm"
      },
      "source": [
        "\n",
        "query_set_embeddings = get_embeddings(tf.reshape(query_set, [num_classes * num_query_points, img_height, img_width, channels]), h_dim, z_dim, reuse=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAC2Ckcf7l9"
      },
      "source": [
        "\n",
        "def euclidean_distance(a, b):\n",
        "\n",
        "    N, D = tf.shape(a)[0], tf.shape(a)[1]\n",
        "    M = tf.shape(b)[0]\n",
        "    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))\n",
        "    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))\n",
        "    return tf.reduce_mean(tf.square(a - b), axis=2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT8tRzE6f99U"
      },
      "source": [
        "distance = euclidean_distance(class_prototype,query_set_embeddings)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbnC0RgugAXU"
      },
      "source": [
        "predicted_probability = tf.reshape(tf.nn.log_softmax(-distance), [num_classes, num_query_points, -1])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpmQRqeT2wY8"
      },
      "source": [
        "#### Compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VGO8WD1gCzs"
      },
      "source": [
        "loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_one_hot, predicted_probability), axis=-1), [-1]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIhFw92y2yOf"
      },
      "source": [
        "#### Calculate accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbdGoNcSgFYM",
        "outputId": "9265bc0f-8101-4515-e6ff-4a31cd6d205e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(predicted_probability, axis=-1), y)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-8b342a457945>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQltqWS20dO"
      },
      "source": [
        "#### We use Adam optimizer for minimizing the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynt2KxFAgHoM",
        "outputId": "538c859d-d502-4f96-83df-9f8682e642f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc4ehPMu22-a"
      },
      "source": [
        "#### we start our tensorflow session and train the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkqHS-6qgKSc"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYJYrRFTgMrl"
      },
      "source": [
        "num_epochs = 20\n",
        "num_episodes = 100"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv-RuYelgO99",
        "outputId": "9f3f2f0d-edcb-49b2-95f5-f26511d636ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        \n",
        "        # select 60 classes\n",
        "        episodic_classes = np.random.permutation(no_of_classes)[:num_way]\n",
        "        \n",
        "        support = np.zeros([num_way, num_shot, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        query = np.zeros([num_way, num_query, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        for index, class_ in enumerate(episodic_classes):\n",
        "            selected = np.random.permutation(num_examples)[:num_shot + num_query]\n",
        "            support[index] = train_dataset[class_, selected[:num_shot]]\n",
        "            \n",
        "            # 5 querypoints per classs\n",
        "            query[index] = train_dataset[class_, selected[num_shot:]]\n",
        "            \n",
        "        support = np.expand_dims(support, axis=-1)\n",
        "        query = np.expand_dims(query, axis=-1)\n",
        "        labels = np.tile(np.arange(num_way)[:, np.newaxis], (1, num_query)).astype(np.uint8)\n",
        "        _, loss_, accuracy_ = sess.run([train, loss, accuracy], feed_dict={support_set: support, query_set: query, y:labels})\n",
        "        \n",
        "        if (episode+1) % 10 == 0:\n",
        "            print('Epoch {} : Episode {} : Loss: {}, Accuracy: {}'.format(epoch+1, episode+1, loss_, accuracy_))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 1 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 2 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 3 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 4 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 5 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 6 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 7 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 8 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 9 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 10 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 11 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 12 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 13 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 14 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 15 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 16 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 17 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 18 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 19 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 10 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 20 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 30 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 40 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 50 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 60 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 70 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 80 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 90 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n",
            "Epoch 20 : Episode 100 : Loss: 5.703781604766846, Accuracy: 0.01666666753590107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZAKCwT9gRXk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}